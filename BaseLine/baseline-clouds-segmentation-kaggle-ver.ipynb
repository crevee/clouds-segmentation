{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"캐글 데이터 경로 확인","metadata":{"id":"xJ_5vemHnXlZ"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"g-1-veNcnW6U","outputId":"1aeb2940-0bdd-448c-ed96-0baf7b8d0064","execution":{"iopub.status.busy":"2022-04-12T08:34:14.702924Z","iopub.execute_input":"2022-04-12T08:34:14.703297Z","iopub.status.idle":"2022-04-12T08:34:15.395431Z","shell.execute_reply.started":"2022-04-12T08:34:14.703195Z","shell.execute_reply":"2022-04-12T08:34:15.388288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 폴더 경로 설정","metadata":{"id":"G1NrfEiTnlLS"}},{"cell_type":"code","source":"workspace_path = '/kaggle/input/clouds-segmentation'  # 본인의 파일 경로 반영","metadata":{"id":"9oW1RJXWnkxw","execution":{"iopub.status.busy":"2022-04-12T08:34:15.397183Z","iopub.execute_input":"2022-04-12T08:34:15.397434Z","iopub.status.idle":"2022-04-12T08:34:15.400756Z","shell.execute_reply.started":"2022-04-12T08:34:15.397398Z","shell.execute_reply":"2022-04-12T08:34:15.400086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 필요한 패키지 로드","metadata":{"id":"86ea40c7"}},{"cell_type":"code","source":"!pip install albumentations==0.4.6\n!pip install yacs","metadata":{"id":"lRfwuEL2--n-","outputId":"f58dedc2-f681-43bc-9d8d-3e01a7d9195e","execution":{"iopub.status.busy":"2022-04-12T08:34:15.402383Z","iopub.execute_input":"2022-04-12T08:34:15.403095Z","iopub.status.idle":"2022-04-12T08:34:34.231348Z","shell.execute_reply.started":"2022-04-12T08:34:15.403058Z","shell.execute_reply":"2022-04-12T08:34:34.230529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\nimport yaml\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport random\nimport torch.backends.cudnn as cudnn\nimport time\nfrom torchvision import models\nfrom tqdm import tqdm","metadata":{"id":"52bbfdfb","execution":{"iopub.status.busy":"2022-04-12T08:34:34.235453Z","iopub.execute_input":"2022-04-12T08:34:34.235677Z","iopub.status.idle":"2022-04-12T08:34:37.67966Z","shell.execute_reply.started":"2022-04-12T08:34:34.235652Z","shell.execute_reply":"2022-04-12T08:34:37.678943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 재구현 세팅","metadata":{"id":"649609b1"}},{"cell_type":"code","source":"def init_seeds(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html\n    if seed == 0:  # slower, more reproducible\n        cudnn.deterministic = True\n        cudnn.benchmark = False\n    else:  # faster, less reproducible\n        cudnn.deterministic = False\n        cudnn.benchmark = True","metadata":{"id":"c35eec19","execution":{"iopub.status.busy":"2022-04-12T08:34:37.680791Z","iopub.execute_input":"2022-04-12T08:34:37.681688Z","iopub.status.idle":"2022-04-12T08:34:37.686148Z","shell.execute_reply.started":"2022-04-12T08:34:37.681657Z","shell.execute_reply":"2022-04-12T08:34:37.685509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"init_seeds(1)","metadata":{"id":"b90c4ec5","execution":{"iopub.status.busy":"2022-04-12T08:34:37.687925Z","iopub.execute_input":"2022-04-12T08:34:37.688297Z","iopub.status.idle":"2022-04-12T08:34:37.700398Z","shell.execute_reply.started":"2022-04-12T08:34:37.688263Z","shell.execute_reply":"2022-04-12T08:34:37.699741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터 로드","metadata":{"id":"3f8a343a"}},{"cell_type":"code","source":"rgb_path = os.path.join(workspace_path, 'train/rgb/')\nngr_path = os.path.join(workspace_path, 'train/ngr/')\nlabel_path = os.path.join(workspace_path, 'train/label/')","metadata":{"id":"499b3f23","execution":{"iopub.status.busy":"2022-04-12T08:34:37.701555Z","iopub.execute_input":"2022-04-12T08:34:37.701899Z","iopub.status.idle":"2022-04-12T08:34:37.707854Z","shell.execute_reply.started":"2022-04-12T08:34:37.701864Z","shell.execute_reply":"2022-04-12T08:34:37.707072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rgb_images = os.listdir(rgb_path)\nrgb_images = [os.path.join(rgb_path,x) for x in rgb_images]\nngr_images = os.listdir(ngr_path)\nngr_images = [os.path.join(ngr_path, x) for x in ngr_images]\nlabel_images = os.listdir(label_path)\nlabel_images = [os.path.join(label_path, x) for x in label_images]","metadata":{"id":"d4e26b49","execution":{"iopub.status.busy":"2022-04-12T08:34:37.709085Z","iopub.execute_input":"2022-04-12T08:34:37.709382Z","iopub.status.idle":"2022-04-12T08:34:37.725452Z","shell.execute_reply.started":"2022-04-12T08:34:37.709346Z","shell.execute_reply":"2022-04-12T08:34:37.724851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터셋 클래스 정의","metadata":{"id":"997d5237"}},{"cell_type":"code","source":"class CloudDataset(torch.utils.data.Dataset):\n    def __init__(self, image_path, label_path, patch_size = 400, patch_stride = 100, is_train = True, cache_dir = './cache', transforms = None):\n        self.image_path = image_path\n        self.label_path = label_path\n        self.patch_size = patch_size\n        self.patch_stride = patch_stride\n        self.is_train = is_train\n        self.transforms = transforms\n        \n        self.patch_images = []\n        self.patch_labels = []\n        \n        \n        cache_dir = cache_dir\n        os.makedirs(cache_dir, exist_ok=True)\n        if is_train:\n            for img_path in self.image_path:\n                img = cv2.imread(img_path)\n                img_count = 0\n                for x in range(0, img.shape[0]-self.patch_size+1, self.patch_stride):\n                    for y in range(0, img.shape[1]-self.patch_size+1, self.patch_stride):\n                        patch_image = img[x:x+patch_size, y:y+patch_size, :].copy()\n                        patch_path = f'rgb_{os.path.splitext(os.path.basename(img_path))[0]}_{img_count}.png'\n                        if not os.path.isfile(os.path.join(cache_dir, patch_path)):\n                            cv2.imwrite(os.path.join(cache_dir, patch_path), patch_image)\n                        self.patch_images.append(os.path.join(cache_dir, patch_path))\n                        img_count += 1\n\n            for label_path in self.label_path:\n                img = cv2.imread(label_path)\n                img_count = 0\n                for x in range(0, img.shape[0]-self.patch_size+1, self.patch_stride):\n                    for y in range(0, img.shape[1]-self.patch_size+1, self.patch_stride):\n                        patch_image = img[x:x+patch_size, y:y+patch_size, :].copy()\n                        patch_path = f'label_{os.path.splitext(os.path.basename(label_path))[0]}_{img_count}.png'\n                        if not os.path.isfile(os.path.join(cache_dir, patch_path)):\n                            cv2.imwrite(os.path.join(cache_dir, patch_path), patch_image)\n                        self.patch_labels.append(os.path.join(cache_dir, patch_path))\n                        img_count += 1\n        else:\n            self.patch_images = self.image_path\n            self.patch_labels = self.label_path\n    def __len__(self):\n        return len(self.patch_images)\n        \n    def __getitem__(self, idx):\n        img = cv2.imread(self.patch_images[idx])\n        \n        if self.is_train:\n            label = cv2.imread(self.patch_labels[idx])\n            # numpy arrays to tensors\n            h, w = label.shape[:2]\n        \n            target = np.zeros((h, w), dtype=np.uint8)\n            pos = np.where(np.all(label == [0, 0, 255], axis=-1))  # thick cloud\n            target[pos] = 1\n            pos = np.where(np.all(label == [0, 255, 0], axis=-1))  # thin cloud\n            target[pos] = 2\n            pos = np.where(np.all(label == [0, 255, 255], axis=-1))  # cloud shadow\n            target[pos] = 3\n        else:\n            target = img\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n            \n        if self.is_train:\n            return img, target\n        else:\n            return img, self.patch_images[idx]","metadata":{"id":"ca96420c","execution":{"iopub.status.busy":"2022-04-12T08:34:37.726418Z","iopub.execute_input":"2022-04-12T08:34:37.726641Z","iopub.status.idle":"2022-04-12T08:34:37.74719Z","shell.execute_reply.started":"2022-04-12T08:34:37.726609Z","shell.execute_reply":"2022-04-12T08:34:37.745871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 파라미터 세팅","metadata":{"id":"09d79e8b"}},{"cell_type":"code","source":"batch_size = 8\nepochs = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\npatch_size = 400\npatch_stride = 100\nnum_workers = 0\n\nnum_classes = 4\nclass_names = ['thick cloud', 'thin cloud', 'cloud shadow']\n\ntrain_data_rate = 0.7\n\nmodel_name = 'dilated_unet'\n\nloss_func = 'dice'","metadata":{"id":"a6ac4eae","execution":{"iopub.status.busy":"2022-04-12T08:34:37.751151Z","iopub.execute_input":"2022-04-12T08:34:37.751577Z","iopub.status.idle":"2022-04-12T08:34:37.805706Z","shell.execute_reply.started":"2022-04-12T08:34:37.751544Z","shell.execute_reply":"2022-04-12T08:34:37.805048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터증대","metadata":{"id":"7e4513b1"}},{"cell_type":"code","source":"class ImageAug:\n    def __init__(self):\n        self.aug = A.Compose([A.HorizontalFlip(p=0.5),\n                             A.VerticalFlip(p=0.5),\n                             A.ShiftScaleRotate(p=0.5),\n                             A.RandomBrightnessContrast(p=0.3),\n                             A.Normalize(),\n                             ToTensorV2()])\n\n    def __call__(self, img, label):\n        transformed = self.aug(image=img, mask=label)\n        return transformed['image'], transformed['mask']\n\nclass DefaultAug:\n    def __init__(self):\n        self.aug = A.Compose([A.Normalize(),\n                             ToTensorV2()])\n\n    def __call__(self, img, label):\n        transformed = self.aug(image=img, mask=label)\n        return transformed['image'], transformed['mask']","metadata":{"id":"17dc2c12","execution":{"iopub.status.busy":"2022-04-12T08:34:37.806685Z","iopub.execute_input":"2022-04-12T08:34:37.807398Z","iopub.status.idle":"2022-04-12T08:34:37.817653Z","shell.execute_reply.started":"2022-04-12T08:34:37.807358Z","shell.execute_reply":"2022-04-12T08:34:37.816854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = ImageAug()\nval_transforms = DefaultAug()","metadata":{"id":"03a59423","execution":{"iopub.status.busy":"2022-04-12T08:34:37.818905Z","iopub.execute_input":"2022-04-12T08:34:37.819241Z","iopub.status.idle":"2022-04-12T08:34:37.825627Z","shell.execute_reply.started":"2022-04-12T08:34:37.819191Z","shell.execute_reply":"2022-04-12T08:34:37.824734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터셋 정의","metadata":{"id":"2147f462"}},{"cell_type":"code","source":"output_path = '/kaggle/working'\n#train dataset\ntrain_dataset = CloudDataset(rgb_images[:int(len(rgb_images)*train_data_rate)], label_images[:int(len(label_images)*train_data_rate)],\n                            transforms=train_transforms, cache_dir=os.path.join(output_path, 'cache'))\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                                               num_workers=num_workers, pin_memory=True, drop_last=True)\n\n#valid dataset\nval_dataset = CloudDataset(rgb_images[int(len(rgb_images)*train_data_rate):], label_images[int(len(label_images)*train_data_rate):],\n                            transforms=val_transforms, cache_dir=os.path.join(output_path, 'cache'))\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n                                               num_workers=num_workers, pin_memory=True, drop_last=True)","metadata":{"id":"98bed485","execution":{"iopub.status.busy":"2022-04-12T08:34:37.827359Z","iopub.execute_input":"2022-04-12T08:34:37.828164Z","iopub.status.idle":"2022-04-12T08:42:01.457321Z","shell.execute_reply.started":"2022-04-12T08:34:37.828128Z","shell.execute_reply":"2022-04-12T08:42:01.456475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 모델 정의","metadata":{"id":"55cbab53"}},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"id":"e54a2c7d","execution":{"iopub.status.busy":"2022-04-12T08:42:01.45865Z","iopub.execute_input":"2022-04-12T08:42:01.458927Z","iopub.status.idle":"2022-04-12T08:42:01.463637Z","shell.execute_reply.started":"2022-04-12T08:42:01.458891Z","shell.execute_reply":"2022-04-12T08:42:01.462715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass DoubleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConvBlock, self).__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                                   nn.ReLU(inplace=True),\n                                   nn.BatchNorm2d(out_channels),\n                                   nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                                   nn.ReLU(inplace=True),\n                                   nn.BatchNorm2d(out_channels))\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass DilatedConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation, padding):\n        super(DilatedConvBlock, self).__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding, dilation=dilation),\n                                   nn.ReLU(inplace=True),\n                                   nn.BatchNorm2d(out_channels))\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\n\n\nclass ConcatDoubleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConcatDoubleConvBlock, self).__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                                   nn.ReLU(inplace=True),\n                                   nn.BatchNorm2d(out_channels),\n                                   nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                                   nn.ReLU(inplace=True),\n                                   nn.BatchNorm2d(out_channels))\n\n    def forward(self, x, skip):\n        x = torch.cat((skip, x), dim=1)\n        x = self.block(x)\n        return x\n\n\n\nclass MyDilatedConvUNet(nn.Module):\n    def __init__(self, filters=44, depth=3, bottleneck_depth=6):\n        super(MyDilatedConvUNet, self).__init__()\n        self.depth = depth\n        self.encoder_path = nn.ModuleList()\n        src_in_channels = 3     # Geo-TIFF has four channels (R, G, B, and NIR)\n        for d in range(depth):\n            in_channels = src_in_channels if d == 0 else filters * 2 ** (d-1)\n            self.encoder_path.append(\n                DoubleConvBlock(in_channels, filters * 2 ** d))\n        self.maxpool = nn.MaxPool2d(2, 2, padding=0)\n        self.bottleneck_path = nn.ModuleList()\n        for d in range(bottleneck_depth):\n            in_channels = filters * 2 ** (depth - 1) if d == 0 else filters * 2 ** depth\n            self.bottleneck_path.append(DilatedConvBlock(in_channels, filters * 2 ** depth, 2 ** d, 2 ** d))\n        self.decoder_path = nn.ModuleList()\n        for d in range(depth):\n            in_channels = filters * 2 ** (depth - d)\n            self.decoder_path.append(ConcatDoubleConvBlock(in_channels, filters * 2 ** (depth - d - 1)))\n        self.up_path = nn.ModuleList()\n        for d in range(depth):\n            in_channels = filters * 2 ** (depth - d)\n            self.up_path.append(nn.ConvTranspose2d(in_channels, filters * 2 ** (depth - d - 1),\n                                                        kernel_size=4, stride=2, padding=1))\n        out_channels = 4     # output channels (num_classes + 1(background))\n        self.last_conv = nn.Conv2d(filters, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip = []\n        for block in self.encoder_path:\n            x = block(x)\n            skip.append(x)\n            x = self.maxpool(x)\n        dilated = []\n        for block in self.bottleneck_path:\n            x = block(x)\n            dilated.append(x)\n        x = torch.stack(dilated, dim=-1).sum(dim=-1)  # sum over list\n\n        # up-sampling and double convolutions\n        for d in range(self.depth):\n            x = self.up_path[d](x)\n            x = self.decoder_path[d](x, skip[-(d+1)])\n\n        return self.last_conv(x)","metadata":{"id":"eIK90NknzGPn","execution":{"iopub.status.busy":"2022-04-12T08:42:01.465255Z","iopub.execute_input":"2022-04-12T08:42:01.465616Z","iopub.status.idle":"2022-04-12T08:42:01.492309Z","shell.execute_reply.started":"2022-04-12T08:42:01.465585Z","shell.execute_reply":"2022-04-12T08:42:01.491598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport functools\n\nimport torch._utils\nimport torch.nn.functional as F\n\nBatchNorm2d = functools.partial(nn.BatchNorm2d)\n# BatchNorm2d = nn.SynchBatchNorm(InPlaceABNSync, activation='none')\n\nBN_MOMENTUM = 0.01\nlogger = logging.getLogger(__name__)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=False)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=False)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=False)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(inplace=False)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=[height_output, width_output],\n                        mode='bilinear', align_corners=False)\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass HighResolutionNet(nn.Module):\n\n    def __init__(self, config, **kwargs):\n        extra = config.MODEL.EXTRA\n        super(HighResolutionNet, self).__init__()\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=False)\n\n        self.stage1_cfg = extra['STAGE1']\n        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n        block = blocks_dict[self.stage1_cfg['BLOCK']]\n        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n        stage1_out_channel = block.expansion * num_channels\n\n        self.stage2_cfg = extra['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        last_inp_channels = np.int(np.sum(pre_stage_channels))\n\n        self.last_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=last_inp_channels,\n                out_channels=last_inp_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0),\n            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels=last_inp_channels,\n                out_channels=config.DATASET.NUM_CLASSES,\n                kernel_size=extra.FINAL_CONV_KERNEL,\n                stride=1,\n                padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0)\n        )\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=False)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=False)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                if i < self.stage2_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                if i < self.stage3_cfg['NUM_BRANCHES']:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n        x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n        x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n\n        x = torch.cat([x[0], x1, x2, x3], 1)\n\n        x = self.last_layer(x)\n\n        return x\n\n    def init_weights(self, pretrained='', ):\n        logger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)\n            logger.info('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            for k, _ in pretrained_dict.items():\n                logger.info(\n                    '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n\n\ndef get_seg_model(cfg, **kwargs):\n    model = HighResolutionNet(cfg, **kwargs)\n    model.init_weights(cfg.MODEL.PRETRAINED)\n\n    return model","metadata":{"id":"gWNuzVTfzGUf","execution":{"iopub.status.busy":"2022-04-12T08:42:01.494144Z","iopub.execute_input":"2022-04-12T08:42:01.494543Z","iopub.status.idle":"2022-04-12T08:42:01.576314Z","shell.execute_reply.started":"2022-04-12T08:42:01.494506Z","shell.execute_reply":"2022-04-12T08:42:01.575598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yacs.config import CfgNode as CN\n\n_C = CN()\n\n_C.OUTPUT_DIR = ''\n_C.LOG_DIR = ''\n_C.GPUS = (0,)\n_C.WORKERS = 4\n_C.PRINT_FREQ = 20\n_C.AUTO_RESUME = False\n_C.PIN_MEMORY = True\n_C.RANK = 0\n\n# Cudnn related params\n_C.CUDNN = CN()\n_C.CUDNN.BENCHMARK = True\n_C.CUDNN.DETERMINISTIC = False\n_C.CUDNN.ENABLED = True\n\n# common params for NETWORK\n_C.MODEL = CN()\n_C.MODEL.NAME = 'seg_hrnet'\n_C.MODEL.PRETRAINED = ''\n_C.MODEL.EXTRA = CN(new_allowed=True)\n\n_C.LOSS = CN()\n_C.LOSS.USE_OHEM = False\n_C.LOSS.OHEMTHRES = 0.9\n_C.LOSS.OHEMKEEP = 100000\n_C.LOSS.CLASS_BALANCE = True\n\n# DATASET related params\n_C.DATASET = CN()\n_C.DATASET.ROOT = ''\n_C.DATASET.DATASET = 'cityscapes'\n_C.DATASET.NUM_CLASSES = 19\n_C.DATASET.TRAIN_SET = 'list/cityscapes/train.lst'\n_C.DATASET.EXTRA_TRAIN_SET = ''\n_C.DATASET.TEST_SET = 'list/cityscapes/val.lst'\n\n# training\n_C.TRAIN = CN()\n\n_C.TRAIN.IMAGE_SIZE = [1024, 512]  # width * height\n_C.TRAIN.BASE_SIZE = 2048\n_C.TRAIN.DOWNSAMPLERATE = 1\n_C.TRAIN.FLIP = True\n_C.TRAIN.MULTI_SCALE = True\n_C.TRAIN.SCALE_FACTOR = 16\n\n_C.TRAIN.LR_FACTOR = 0.1\n_C.TRAIN.LR_STEP = [90, 110]\n_C.TRAIN.LR = 0.01\n_C.TRAIN.EXTRA_LR = 0.001\n\n_C.TRAIN.OPTIMIZER = 'sgd'\n_C.TRAIN.MOMENTUM = 0.9\n_C.TRAIN.WD = 0.0001\n_C.TRAIN.NESTEROV = False\n_C.TRAIN.IGNORE_LABEL = -1\n\n_C.TRAIN.BEGIN_EPOCH = 0\n_C.TRAIN.END_EPOCH = 484\n_C.TRAIN.EXTRA_EPOCH = 0\n\n_C.TRAIN.RESUME = False\n\n_C.TRAIN.BATCH_SIZE_PER_GPU = 32\n_C.TRAIN.SHUFFLE = True\n# only using some training samples\n_C.TRAIN.NUM_SAMPLES = 0\n\n# testing\n_C.TEST = CN()\n\n_C.TEST.IMAGE_SIZE = [2048, 1024]  # width * height\n_C.TEST.BASE_SIZE = 2048\n\n_C.TEST.BATCH_SIZE_PER_GPU = 32\n# only testing some samples\n_C.TEST.NUM_SAMPLES = 0\n\n_C.TEST.MODEL_FILE = ''\n_C.TEST.FLIP_TEST = False\n_C.TEST.MULTI_SCALE = False\n_C.TEST.SCALE_LIST = [1]\n\n# debug\n_C.DEBUG = CN()\n_C.DEBUG.DEBUG = False\n_C.DEBUG.SAVE_BATCH_IMAGES_GT = False\n_C.DEBUG.SAVE_BATCH_IMAGES_PRED = False\n_C.DEBUG.SAVE_HEATMAPS_GT = False\n_C.DEBUG.SAVE_HEATMAPS_PRED = False\n\n\ndef update_config(file):\n    cfg = _C.clone()\n    cfg.defrost()\n    cfg.merge_from_file(file)\n    cfg.freeze()\n    return cfg","metadata":{"id":"1e3d9a09","execution":{"iopub.status.busy":"2022-04-12T08:42:01.577623Z","iopub.execute_input":"2022-04-12T08:42:01.578298Z","iopub.status.idle":"2022-04-12T08:42:01.600746Z","shell.execute_reply.started":"2022-04-12T08:42:01.578268Z","shell.execute_reply":"2022-04-12T08:42:01.599996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model\nif model_name == 'deeplabv3':\n    model = models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=4)\n#elif model_name == 'hrnet_w18':\n#    hrnet_cfg = update_config(os.path.join(workspace_path, 'hrnet_w18_config.yaml'))\n#    model = get_seg_model(hrnet_cfg)\n#elif model_name == 'hrnet_w48':\n#    hrnet_cfg = update_config(os.path.join(workspace_path, 'hrnet_w48_config.yaml'))\n#    model = get_seg_model(hrnet_cfg)\nelif model_name == 'dilated_unet':\n    model = MyDilatedConvUNet()\n\nmodel.to(device)\n\nprint('number of parameters: ', count_parameters(model))","metadata":{"id":"78338b36","outputId":"0c6e4902-ac7e-4e22-b79f-a6af6efaebcf","execution":{"iopub.status.busy":"2022-04-12T08:42:01.60219Z","iopub.execute_input":"2022-04-12T08:42:01.602507Z","iopub.status.idle":"2022-04-12T08:42:04.591269Z","shell.execute_reply.started":"2022-04-12T08:42:01.602455Z","shell.execute_reply":"2022-04-12T08:42:04.590498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Opimizer 정의","metadata":{"id":"467743f7"}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)","metadata":{"id":"314acf1e","execution":{"iopub.status.busy":"2022-04-12T08:42:04.592372Z","iopub.execute_input":"2022-04-12T08:42:04.592631Z","iopub.status.idle":"2022-04-12T08:42:04.597698Z","shell.execute_reply.started":"2022-04-12T08:42:04.592594Z","shell.execute_reply":"2022-04-12T08:42:04.59658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 필요 함수 정의","metadata":{"id":"QuJfaks1zbtC"}},{"cell_type":"code","source":"def fitness_test(true, pred, num_classes=4):\n    eps = 1e-7\n    true_one_hot = F.one_hot(true.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n    true_one_hot = true_one_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n    pred_max = pred.argmax(1)      # (B, C, H, W) to (B, H, W)\n    pix_acc = (true == pred_max.unsqueeze(1)).sum().float().div(true.nelement())\n    pred_one_hot = F.one_hot(pred_max, num_classes=num_classes)   # (B, H, W) to (B, H, W, C)\n    pred_one_hot = pred_one_hot.permute(0, 3, 1, 2)   # (B, H, W, C) to (B, C, H, W)\n\n    true_one_hot = true_one_hot.type(pred_one_hot.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))  # dims = (0, 2, 3)\n    intersection = torch.sum(pred_one_hot & true_one_hot, dims)\n    union = torch.sum(pred_one_hot | true_one_hot, dims)\n    m_iou = (intersection / (union + eps)).mean()\n\n    return m_iou.item(), pix_acc.item()","metadata":{"id":"c124f5df","execution":{"iopub.status.busy":"2022-04-12T08:42:04.599127Z","iopub.execute_input":"2022-04-12T08:42:04.600142Z","iopub.status.idle":"2022-04-12T08:42:04.609595Z","shell.execute_reply.started":"2022-04-12T08:42:04.600107Z","shell.execute_reply":"2022-04-12T08:42:04.608858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss 함수 정의\ndef ce_loss(true, logits, ignore=255):\n    \"\"\"Computes the weighted multi-class cross-entropy loss.\n    Args:\n        true: a tensor of shape [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        ignore: the class index to ignore.\n    Returns:\n        ce_loss: the weighted multi-class cross-entropy loss.\n    \"\"\"\n    ce_loss = F.cross_entropy(\n        logits.float(),\n        true.squeeze(1).long(),    # [B, H, W]\n        ignore_index=ignore,\n    )\n    return ce_loss\n\n\ndef dice_loss(true, logits, eps=1e-7):\n    \"\"\"Computes the Sørensen–Dice loss.\n    Note that PyTorch optimizers minimize a loss. In this\n    case, we would like to maximize the dice loss so we\n    return the negated dice loss.\n    Args:\n        true: a tensor of shape [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        eps: added to the denominator for numerical stability.\n    Returns:\n        dice_loss: the Sørensen–Dice loss.\n    \"\"\"\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        # true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n        true_1_hot = F.one_hot(true.squeeze(1), num_classes=num_classes)   # (B, 1, H, W) to (B, H, W, C)\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2)                        # (B, H, W, C) to (B, C, H, W)\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type()).contiguous()\n    dims = (0,) + tuple(range(2, true.ndimension()))        # dims = (0, 2, 3)\n    intersection = torch.sum(probas * true_1_hot, dims)     # intersection w.r.t. the class\n    cardinality = torch.sum(probas + true_1_hot, dims)      # cardinality w.r.t. the class\n    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n    return (1 - dice_loss)\n\n\ndef jaccard_loss(true, logits, eps=1e-7):\n    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n    Note that PyTorch optimizers minimize a loss. In this\n    case, we would like to maximize the jaccard loss so we\n    return the negated jaccard loss.\n    Args:\n        true: a tensor of shape [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        eps: added to the denominator for numerical stability.\n    Returns:\n        jacc_loss: the Jaccard loss.\n    \"\"\"\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        true_1_hot = F.one_hot(true.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type()).contiguous()\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return (1 - jacc_loss)","metadata":{"id":"v74FWycdzdev","execution":{"iopub.status.busy":"2022-04-12T08:42:04.612428Z","iopub.execute_input":"2022-04-12T08:42:04.612628Z","iopub.status.idle":"2022-04-12T08:42:04.629985Z","shell.execute_reply.started":"2022-04-12T08:42:04.612605Z","shell.execute_reply":"2022-04-12T08:42:04.629099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습 함수 정의","metadata":{"id":"9d1af165"}},{"cell_type":"code","source":"def train(model, optimizer, train_dataloader, val_dataloader, loss_func, epochs, device, patch_size=400, use_scheduler=False, save_path='./ckpt'):\n\n    # Learning rate scheduler\n    if use_scheduler:\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=1)\n    else:\n        lr_scheduler = None\n    start_epoch = 0\n    resume = True\n\n    if not os.path.isdir(save_path):\n        os.mkdir(save_path)\n\n    weight_file = save_path + '/{}.pt'.format(model_name)\n\n    best_fit = 0.0\n    num_epochs = epochs\n\n    if resume:\n        if os.path.exists(weight_file):\n            checkpoint = torch.load(weight_file)\n            model.load_state_dict(checkpoint['model'])\n            start_epoch = checkpoint['epoch'] + 1\n            best_fit = checkpoint['best_fit']\n            print(\"Starting training for %g epochs...\" % start_epoch)\n\n    # Start training\n\n    for epoch in range(start_epoch, num_epochs):\n        # loss, metric = train_one_epoch(model, optimizer, dataloader, device, epoch)\n        t0 = time.time()\n        loss = train_one_epoch(model, optimizer, train_dataloader, loss_func, device, epoch, num_epochs)\n        t1 = time.time()\n        print('[Epoch %g] loss=%.4f, time=%.1f' % (epoch, loss.item(), t1 - t0))\n        if lr_scheduler is not None:\n            lr_scheduler.step(loss)\n        #tb_writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n\n        state = {'model_name': model_name, 'epoch': epoch, 'best_fit': best_fit, 'model': model.state_dict()}\n        torch.save(state, weight_file)\n\n        #tb_writer.add_scalar('train_epoch_loss', loss, epoch)\n\n        # validation\n        patch_size = patch_size\n        fit = val_one_epoch(model, val_dataloader, device, epoch, num_epochs, patch_size)\n        if fit > best_fit:\n            print(\"best fit so far=>saved\")\n            torch.save(state, os.path.join(save_path, '{}_best.pt'.format(model_name)))\n            best_fit = fit\n\n\ndef train_one_epoch(model, optimizer, data_loader, loss_func, device, epoch, num_epochs):\n    model.train()\n    losses = np.array([])\n    metrics = np.array([])\n    bi0 = epoch * len(data_loader)  # batch index\n\n    print(('\\n' + '%10s' * 2) % ('Epoch', 'loss'))\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    s = ('%10s' + '%10.4f') % (\n        '-/%g' % (num_epochs - 1), 0.0)\n    pbar.set_description(s)\n    for i, (imgs, targets) in pbar:\n        imgs, targets = imgs.to(device), targets.to(device)\n        if model_name == 'deeplabv3':\n            preds = model(imgs)['out']\n            targets = targets.long()\n        elif model_name == 'hrnet_w18' or model_name == 'hrnet_w48':\n            preds = model(imgs)\n            h, w = preds.shape[2], preds.shape[3]\n            targets = F.interpolate(targets.float(), size=(h, w), mode='nearest').long()\n        elif model_name == 'dilated_unet':\n            preds = model(imgs)\n            targets = targets.long()\n            \n        if loss_func == 'jaccard':\n            loss = jaccard_loss(targets, preds)\n        elif loss_func == 'dice':\n            loss = dice_loss(targets, preds)\n        elif loss_func == 'ce':\n            loss = ce_loss(targets, preds)\n        else:\n            print('unsupported loss function')\n            exit(1)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            # cv2_imshow(imgs[0], preds[0])\n            losses = np.append(losses, loss.item())\n\n            s = ('%10s' + '%10.4f') % (\n                '%g/%g' % (epoch, num_epochs - 1), loss.item())\n            pbar.set_description(s)\n            bi = bi0 + i\n            #tb_writer.add_scalar('train_batch_loss', loss.item(), bi)\n\n    epoch_loss = losses.mean()\n\n    return epoch_loss\n\n\ndef val_one_epoch(model, data_loader, device, epoch, num_epochs, patch_size):\n    model.eval()\n    m_iou_list = np.array([])\n    pix_acc_list = np.array([])\n\n    print(('\\n' + '%10s' * 3) % ('Epoch(V)', 'mIOU', 'Accuracy'))\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    s = ('%10s' + '%10.4f' + ' %8.4f') % (\n        '-/%g' % (num_epochs - 1), 0.0, 0.0)\n    pbar.set_description(s)\n\n    for i, (imgs, targets) in pbar:\n        imgs, targets = imgs.to(device), targets.to(device)\n        with torch.no_grad():\n            if model_name == 'deeplabv3':\n                preds = model(imgs)['out']\n                targets = targets.long()\n            elif model_name == 'hrnet_w18' or model_name == 'hrnet_w48':\n                preds = model(imgs)\n                h, w = preds.shape[2], preds.shape[3]\n                targets = F.interpolate(targets.float(), size=(h, w), mode='nearest').long()\n            elif model_name == 'dilated_unet':\n                preds = model(imgs)\n                targets = targets.long()\n\n            m_iou, pix_acc = fitness_test(targets, preds)\n\n            s = ('%10s' + '%10.4f' + ' %8.4f') % (\n                '%g/%g' % (epoch, num_epochs - 1), m_iou, pix_acc)\n            pbar.set_description(s)\n            m_iou_list = np.append(m_iou_list, m_iou)\n            pix_acc_list = np.append(pix_acc_list, pix_acc)\n    val_m_iou_mean = m_iou_list.mean()\n    val_pix_acc_mean = pix_acc_list.mean()\n    print('[V] mIOU={:.3f}, Accuracy={:.3f}'.format(val_m_iou_mean, val_pix_acc_mean))\n    #tb_writer.add_scalar('val_epoch_m_iou', val_m_iou_mean, epoch)\n    #tb_writer.add_scalar('val_epoch_pix_acc', val_pix_acc_mean, epoch)\n    return val_pix_acc_mean\n","metadata":{"id":"bad17d99","execution":{"iopub.status.busy":"2022-04-12T08:42:04.63154Z","iopub.execute_input":"2022-04-12T08:42:04.632185Z","iopub.status.idle":"2022-04-12T08:42:04.661662Z","shell.execute_reply.started":"2022-04-12T08:42:04.632149Z","shell.execute_reply":"2022-04-12T08:42:04.660862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습 시작","metadata":{"id":"600d9790"}},{"cell_type":"code","source":"train(model, optimizer, train_dataloader, val_dataloader, loss_func, epochs, device, patch_size=patch_size, save_path=os.path.join(output_path, 'ckpt'))","metadata":{"id":"b74e3dff","outputId":"b4ce8779-71be-4428-a357-78010400d46a","execution":{"iopub.status.busy":"2022-04-12T08:46:54.763808Z","iopub.execute_input":"2022-04-12T08:46:54.764284Z","iopub.status.idle":"2022-04-12T17:20:39.080818Z","shell.execute_reply.started":"2022-04-12T08:46:54.764247Z","shell.execute_reply":"2022-04-12T17:20:39.078885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 최고 성능 모델 로드","metadata":{"id":"f8c906b3"}},{"cell_type":"code","source":"save_path=os.path.join(output_path, 'ckpt')\n\ncheckpoint_path = os.path.join(save_path,'{}_best.pt'.format(model_name))\ncheckpoint = torch.load(checkpoint_path)\n\nmodel.load_state_dict(checkpoint['model'])\nmodel.to(device)\n\nprint('model load success')","metadata":{"id":"1f298c14","execution":{"iopub.status.busy":"2022-04-12T17:20:39.132384Z","iopub.status.idle":"2022-04-12T17:20:39.133299Z","shell.execute_reply.started":"2022-04-12T17:20:39.133034Z","shell.execute_reply":"2022-04-12T17:20:39.133069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 테스트 데이터셋 정의","metadata":{"id":"74453fb5"}},{"cell_type":"code","source":"test_rgb_path = os.path.join(workspace_path, 'test/rgb')\ntest_rgb_images = os.listdir(test_rgb_path)\ntest_rgb_images = [os.path.join(test_rgb_path, x) for x in test_rgb_images]","metadata":{"id":"42e6ae95","execution":{"iopub.status.busy":"2022-04-12T17:20:39.13505Z","iopub.status.idle":"2022-04-12T17:20:39.136028Z","shell.execute_reply.started":"2022-04-12T17:20:39.135776Z","shell.execute_reply":"2022-04-12T17:20:39.135802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#empty value\ntest_label_path = os.path.join(workspace_path, 'test/label')\ntry:\n    test_label_images = os.listdir(test_label_path)\nexcept:\n    test_label_images = []\ntest_label_images = [os.path.join(test_label_path, x) for x in test_label_images]","metadata":{"id":"dbeb921a","execution":{"iopub.status.busy":"2022-04-12T17:20:39.137377Z","iopub.status.idle":"2022-04-12T17:20:39.138485Z","shell.execute_reply.started":"2022-04-12T17:20:39.137991Z","shell.execute_reply":"2022-04-12T17:20:39.138017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = CloudDataset(test_rgb_images, test_label_images,\n                            transforms=val_transforms, is_train=False)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,\n                                               num_workers=num_workers, pin_memory=True, drop_last=True)","metadata":{"id":"1ba27f1e","execution":{"iopub.status.busy":"2022-04-12T17:20:39.140127Z","iopub.status.idle":"2022-04-12T17:20:39.14102Z","shell.execute_reply.started":"2022-04-12T17:20:39.140732Z","shell.execute_reply":"2022-04-12T17:20:39.140758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 테스트 결과 저장","metadata":{"id":"5a055ede"}},{"cell_type":"code","source":"model.eval()\n\nresult_path = os.path.join(output_path, 'results')\nos.makedirs(result_path, exist_ok=True)\n\nwith torch.no_grad():\n    pbar = tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n    for i, (imgs, img_path) in pbar:\n        imgs = imgs.to(device)\n        if model_name == 'deeplabv3':\n            preds = model(imgs)['out']\n        elif model_name == 'hrnet_w18' or model_name == 'hrnet_w48':\n            preds = model(imgs)\n            h, w = preds.shape[2], preds.shape[3]\n        elif model_name == 'dilated_unet':\n            preds = model(imgs)\n        \n        pred_img = np.zeros((*list(preds.shape[2:]), 3), dtype=np.uint8)\n        _, idx = preds.squeeze(0).max(0)\n        pos = idx == 0\n        pred_img[pos.cpu().numpy()] = [0, 0, 0]\n        pos = idx == 1\n        pred_img[pos.cpu().numpy()] = [0, 0, 255]\n        pos = idx == 2\n        pred_img[pos.cpu().numpy()] = [0, 255, 0]\n        pos = idx == 3\n        pred_img[pos.cpu().numpy()] = [0, 255, 255]\n        \n        cv2.imwrite(os.path.join(result_path, os.path.basename(img_path[0])), pred_img)\n","metadata":{"id":"eadca7d5","execution":{"iopub.status.busy":"2022-04-12T17:20:39.142675Z","iopub.status.idle":"2022-04-12T17:20:39.143354Z","shell.execute_reply.started":"2022-04-12T17:20:39.143097Z","shell.execute_reply":"2022-04-12T17:20:39.143123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run-Length Encoding","metadata":{"id":"18494383"}},{"cell_type":"code","source":"import pandas as pd","metadata":{"id":"1e8d274d","execution":{"iopub.status.busy":"2022-04-12T17:20:39.144626Z","iopub.status.idle":"2022-04-12T17:20:39.145528Z","shell.execute_reply.started":"2022-04-12T17:20:39.145045Z","shell.execute_reply":"2022-04-12T17:20:39.145085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formatted\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"id":"f4ebad00","execution":{"iopub.status.busy":"2022-04-12T17:20:39.146994Z","iopub.status.idle":"2022-04-12T17:20:39.147691Z","shell.execute_reply.started":"2022-04-12T17:20:39.147445Z","shell.execute_reply":"2022-04-12T17:20:39.14747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_label_file_list = os.listdir(result_path)\ntest_label_path_list = [os.path.join(result_path, x) for x in test_label_file_list]","metadata":{"id":"cd9e1cbb","execution":{"iopub.status.busy":"2022-04-12T17:20:39.148966Z","iopub.status.idle":"2022-04-12T17:20:39.14961Z","shell.execute_reply.started":"2022-04-12T17:20:39.149365Z","shell.execute_reply":"2022-04-12T17:20:39.14939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rle_list = []\nfor file_path in test_label_path_list:\n    img = cv2.imread(file_path)\n    rle = mask2rle(img)\n    rle_list.append(rle)","metadata":{"id":"92c7170e","execution":{"iopub.status.busy":"2022-04-12T17:20:39.150879Z","iopub.status.idle":"2022-04-12T17:20:39.151575Z","shell.execute_reply.started":"2022-04-12T17:20:39.151294Z","shell.execute_reply":"2022-04-12T17:20:39.151321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(os.path.join(workspace_path, 'sample_submission.csv'))","metadata":{"id":"9851d2ed","execution":{"iopub.status.busy":"2022-04-12T17:20:39.15319Z","iopub.status.idle":"2022-04-12T17:20:39.153862Z","shell.execute_reply.started":"2022-04-12T17:20:39.153587Z","shell.execute_reply":"2022-04-12T17:20:39.153612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Image_Label'] = test_label_file_list\nsubmission['EncodedPixels'] = rle_list","metadata":{"id":"b0873d66","execution":{"iopub.status.busy":"2022-04-12T17:20:39.155091Z","iopub.status.idle":"2022-04-12T17:20:39.155734Z","shell.execute_reply.started":"2022-04-12T17:20:39.155489Z","shell.execute_reply":"2022-04-12T17:20:39.155514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('sample_submission.csv', index=False)","metadata":{"id":"7c595d0d","execution":{"iopub.status.busy":"2022-04-12T17:20:39.156993Z","iopub.status.idle":"2022-04-12T17:20:39.157601Z","shell.execute_reply.started":"2022-04-12T17:20:39.157371Z","shell.execute_reply":"2022-04-12T17:20:39.157394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('label', 'zip', result_path)","metadata":{"id":"0e74acfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.rmtree(os.path.join(output_path, 'cache'))\nshutil.rmtree(result_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}